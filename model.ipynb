{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 3000\n",
    "d_model = 512\n",
    "d_pff = 512 * 4\n",
    "max_len = 100\n",
    "dropout = 0.1\n",
    "batch_size = 16\n",
    "n_heads = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, dropout, use_positional_embedding=False):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.use_positional_embedding = use_positional_embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if self.use_positional_embedding:\n",
    "            self.positional_embedding = nn.Embedding(max_len, d_model)\n",
    "        else:\n",
    "            self.positional_encoding = self.get_positional_encoding(self.max_len, self.d_model)\n",
    "            self.register_buffer('positional_encoding_buffer', self.positional_encoding)\n",
    "\n",
    "    def get_positional_encoding(self, max_len, d_model):\n",
    "        positional_encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "\n",
    "        return positional_encoding\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input : (batch_size, max_len)\n",
    "        output : (batch_size, max_len, d_model) \n",
    "        \"\"\"\n",
    "        embedding = self.embedding(x)\n",
    "        if self.use_positional_embedding:\n",
    "            embedding = embedding + self.positional_embedding(torch.arange())\n",
    "        else:\n",
    "            embedding = embedding + self.positional_encoding\n",
    "        \n",
    "        return self.dropout(embedding)\n",
    "\n",
    "# Test the revised class\n",
    "embedding = TokenEmbedding(vocab_size, d_model, max_len, dropout)\n",
    "\n",
    "# Forward pass\n",
    "sample_input = torch.randint(0, vocab_size, (batch_size, max_len))\n",
    "output = embedding(sample_input)\n",
    "\n",
    "# Check the output shape [batch_size, seq_len, d_model]\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100, 512])\n",
      "torch.Size([16, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.n_heads = n_heads # Number of heads\n",
    "        # Make sure d_model is divisible by n_heads\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        self.d_k = d_model // n_heads # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def attention(self, query, key, value, mask=None):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, n_heads, seq_len, d_k) --> (batch, n_heads, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 1, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, n_heads, seq_len, seq_len) # Apply softmax\n",
    "        attention_scores = self.dropout(attention_scores)\n",
    "        # (batch, n_heads, seq_len, seq_len) --> (batch, n_heads, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return attention_scores @ value\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # Split heads\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, n_heads, d_k) --> (batch, n_heads, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        attn_out = self.attention(query, key, value, mask)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, n_heads, seq_len, d_k) --> (batch, seq_len, n_heads, d_k) --> (batch, seq_len, d_model)\n",
    "        attn_out = attn_out.transpose(1, 2).contiguous().view(attn_out.shape[0], -1, self.d_model)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(attn_out)\n",
    "\n",
    "mha = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "mha_out = mha(output, output, output)\n",
    "print(output.shape)\n",
    "print(mha_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PointwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_pff, dropout):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_pff)\n",
    "        self.linear_2 = nn.Linear(d_pff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_2(self.dropout(self.relu(self.linear_1(x))))\n",
    "        return self.dropout(x)\n",
    "\n",
    "pff = PointwiseFeedForward(d_model, d_pff, dropout)\n",
    "pff(output).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementing the single EncoderLayer using the MultiHeadAttention and PointwiseFeedForward sub-layers\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_pff, dropout, norm_first=True):\n",
    "        super().__init__()\n",
    "        self.norm_first = norm_first\n",
    "        \n",
    "        # Multi-Head Attention layer\n",
    "        self.mha = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        \n",
    "        # Pointwise Feed-Forward layer\n",
    "        self.pff = PointwiseFeedForward(d_model, d_pff, dropout)\n",
    "        \n",
    "        # Layer Normalization layers\n",
    "        self.layernorm_1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm_2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src, src_mask=None):\n",
    "        # Multi-Head Attention sub-layer\n",
    "        if self.norm_first:\n",
    "            norm_src = self.layernorm_1(src)\n",
    "            attn_out = self.mha(norm_src, norm_src, norm_src, src_mask)\n",
    "            attn_out = src + attn_out\n",
    "        else:\n",
    "            attn_out = self.dropout(self.mha(src, src, src, src_mask))\n",
    "            attn_out = self.layernorm_1(src + attn_out)\n",
    "        \n",
    "        # Pointwise Feed-Forward sub-layer\n",
    "        if self.norm_first:\n",
    "            pff_out = self.dropout(self.pff(self.layernorm_2(attn_out)))\n",
    "            enc_layer_out = attn_out + pff_out\n",
    "        else:\n",
    "            pff_out = self.dropout(self.pff(attn_out))\n",
    "            enc_layer_out = self.layernorm_2(attn_out + pff_out)\n",
    "\n",
    "        return enc_layer_out\n",
    "\n",
    "enc_layer = EncoderLayer(d_model, n_heads, d_pff, dropout)\n",
    "enc_layer_out = enc_layer(output)\n",
    "enc_layer_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_pff, dropout, norm_first=True):\n",
    "        super().__init__()\n",
    "        self.norm_first = norm_first\n",
    "        # Masked Multi-Head Attention layer\n",
    "        self.masked_mha = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        \n",
    "        # Multi-Head Attention layer (Encoder-Decoder Attention)\n",
    "        self.enc_dec_mha = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        \n",
    "        # Pointwise Feed-Forward layer\n",
    "        self.pff = PointwiseFeedForward(d_model, d_pff, dropout)\n",
    "        \n",
    "        # Layer Normalization layers\n",
    "        self.layernorm_1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm_2 = nn.LayerNorm(d_model)\n",
    "        self.layernorm_3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Single Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, enc_out, src_mask=None, tgt_mask=None):\n",
    "        # Masked Multi-Head Attention sub-layer\n",
    "        if self.norm_first:\n",
    "            norm_tgt = self.layernorm_1(tgt)\n",
    "            masked_attn_out = self.dropout(self.masked_mha(norm_tgt, norm_tgt, norm_tgt, tgt_mask))\n",
    "            masked_attn_out = tgt + masked_attn_out\n",
    "        else:\n",
    "            masked_attn_out = self.dropout(self.masked_mha(tgt, tgt, tgt, tgt_mask))\n",
    "            masked_attn_out = self.layernorm_1(tgt + masked_attn_out)  # Add & Norm\n",
    "\n",
    "        # Multi-Head Attention sub-layer (Encoder-Decoder Attention)\n",
    "        if self.norm_first:\n",
    "            enc_dec_attn_out = self.dropout(self.enc_dec_mha(self.layernorm_2(masked_attn_out),\n",
    "                                                             enc_out, enc_out, src_mask))\n",
    "            enc_dec_attn_out = masked_attn_out + enc_dec_attn_out\n",
    "        else:\n",
    "            enc_dec_attn_out = self.dropout(self.enc_dec_mha(masked_attn_out, enc_out, enc_out, src_mask))\n",
    "            enc_dec_attn_out = self.layernorm_2(masked_attn_out + enc_dec_attn_out)\n",
    "\n",
    "        # Pointwise Feed-Forward sub-layer\n",
    "        if self.norm_first:\n",
    "            pff_out = self.dropout(self.pff(self.layernorm_3(enc_dec_attn_out)))\n",
    "            dec_layer_out = enc_dec_attn_out + pff_out\n",
    "        else:\n",
    "            pff_out = self.dropout(self.pff(enc_dec_attn_out))\n",
    "            dec_layer_out = self.layernorm_3(enc_dec_attn_out + pff_out)  # Add & Norm\n",
    "        \n",
    "        return dec_layer_out\n",
    "\n",
    "dec_layer = DecoderLayer(d_model, n_heads, d_pff, dropout)\n",
    "dec_layer(output, enc_layer_out).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- src_embedding\n",
    "- tgt_embedding\n",
    "- encoder\n",
    "- decoder\n",
    "- generator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
