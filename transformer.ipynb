{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "![](./images/model-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 3000\n",
    "tgt_vocab_size = 3000\n",
    "d_model = 512\n",
    "d_pff = 512 * 4\n",
    "max_len = 100\n",
    "dropout = 0.1\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 max_len,\n",
    "                 dropout,\n",
    "                 use_positional_embedding=False):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.use_positional_embedding = use_positional_embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 위치 정보를 표현하기 위해\n",
    "        # 고정된 positional encoding 값을 사용할지\n",
    "        # 학습 가능한 positional embedding을 사용할지\n",
    "        if self.use_positional_embedding:\n",
    "            self.positional_embedding = nn.Embedding(max_len, d_model)\n",
    "        else:\n",
    "            self.positional_encoding = self.get_positional_encoding(self.max_len, self.d_model)\n",
    "            self.positional_encoding.requires_grad_(False)\n",
    "            self.register_buffer('positional_encoding_buffer', self.positional_encoding)\n",
    "\n",
    "    def get_positional_encoding(self, max_len, d_model):\n",
    "        positional_encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "\n",
    "        return positional_encoding\n",
    "        \n",
    "    def forward(self, token):\n",
    "        # input : (batch, max_len)\n",
    "        # output : (batch, max_len, d_model)\n",
    "        embedding = self.embedding(token)\n",
    "        if self.use_positional_embedding:\n",
    "            positions = torch.arange(0, token.size(1)).expand(token.size(0), token.size(1)).to(token.device)\n",
    "            embedding = embedding + self.positional_embedding(positions)\n",
    "        else:\n",
    "            embedding = embedding + self.positional_encoding[:, :token.size(1), :]\n",
    "\n",
    "        return self.dropout(embedding)\n",
    "\n",
    "\n",
    "embedding = TokenEmbedding(src_vocab_size, d_model, max_len, dropout)\n",
    "\n",
    "input = torch.randint(0, src_vocab_size, (batch_size, max_len))\n",
    "output = embedding(input)\n",
    "\n",
    "print(input.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "`A robot must obey the orders given it`\n",
    "![](./images/self-attention-example-folders-3.png)\n",
    "![](./images/self-attention-example-folders-scores-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Query's Attention}\\left( Q, K, V \\right) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V$\n",
    "![](./images/attention.png)\n",
    "![](./images/self-attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100, 512])\n",
      "torch.Size([16, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        # d_model이 n_heads로 나누어떨어져야 함\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def attention(self, query, key, value, mask=None):\n",
    "        d_k = query.shape[-1]\n",
    "        # (batch, n_heads, max_len, d_k) -> (batch, n_heads, max_len, max_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 1\n",
    "            attention_scores.masked_fill_(mask == 1, -1e9)\n",
    "        # Apply softmax\n",
    "        # (batch, n_heads, max_len, max_len) \n",
    "        attention_scores = F.softmax(attention_scores, dim=-1) \n",
    "\n",
    "        # (batch, n_heads, max_len, max_len) --> (batch, n_heads, max_len, d_k)\n",
    "        return attention_scores @ value\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        query = self.w_q(q) # (batch, max_len, d_model) --> (batch, max_len, d_model)\n",
    "        key = self.w_k(k) # (batch, max_len, d_model) --> (batch, max_len, d_model)\n",
    "        value = self.w_v(v) # (batch, max_len, d_model) --> (batch, max_len, d_model)\n",
    "\n",
    "        # Split heads\n",
    "        # (batch, max_len, d_model) --> (batch, max_len, n_heads, d_k) --> (batch, n_heads, max_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        attn_out = self.attention(query, key, value, mask)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, n_heads, max_len, d_k) --> (batch, max_len, n_heads, d_k) --> (batch, max_len, d_model)\n",
    "        attn_out = attn_out.transpose(1, 2).contiguous().view(attn_out.shape[0], -1, self.d_model)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, max_len, d_model) --> (batch, max_len, d_model)\n",
    "        attn_out = self.w_o(attn_out)\n",
    "        return self.dropout(attn_out)\n",
    "\n",
    "mha = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "mha_out = mha(output, output, output)\n",
    "print(output.shape)\n",
    "print(mha_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PointwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_pff, dropout):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_pff)\n",
    "        self.linear_2 = nn.Linear(d_pff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_2(self.dropout(self.relu(self.linear_1(x))))\n",
    "        return self.dropout(x)\n",
    "\n",
    "pff = PointwiseFeedForward(d_model, d_pff, dropout)\n",
    "pff(output).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100, 512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_pff, dropout, norm_first=True):\n",
    "        super().__init__()\n",
    "        self.norm_first = norm_first\n",
    "        \n",
    "        # Multi-Head Attention layer\n",
    "        self.mha = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        \n",
    "        # Pointwise Feed-Forward layer\n",
    "        self.pff = PointwiseFeedForward(d_model, d_pff, dropout)\n",
    "        \n",
    "        # Layer Normalization layers\n",
    "        self.layernorm_1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm_2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, src, src_mask=None):\n",
    "        # Multi-Head Attention sub-layer\n",
    "        if self.norm_first:\n",
    "            norm_src = self.layernorm_1(src)\n",
    "            attn_out = src + self.mha(norm_src, norm_src, norm_src, src_mask)\n",
    "            norm_attn_out = self.layernorm_2(attn_out)\n",
    "            pff_out = attn_out + self.pff(norm_attn_out)\n",
    "        else:\n",
    "            attn_out = self.mha(src, src, src, src_mask)\n",
    "            attn_out = self.layernorm_1(src + attn_out)\n",
    "            pff_out = self.pff(attn_out)\n",
    "            pff_out = self.layernorm_2(attn_out + pff_out)\n",
    "\n",
    "        return pff_out\n",
    "\n",
    "enc_layer = EncoderLayer(d_model, n_heads, d_pff, dropout)\n",
    "enc_layer_out = enc_layer(output)\n",
    "enc_layer_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100, 512])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, d_model, n_heads, d_pff, max_len, n_layers, dropout, norm_first=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.norm_first = norm_first\n",
    "        if norm_first:\n",
    "            self.layernorm = nn.LayerNorm(d_model)\n",
    "        # Source Token Embedding\n",
    "        self.src_embedding = TokenEmbedding(src_vocab_size, d_model, max_len, dropout)\n",
    "        \n",
    "        # Encoder Layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, n_heads, d_pff, dropout, norm_first=norm_first)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, src, src_mask=None):\n",
    "        # Source Token Embedding\n",
    "        src = self.src_embedding(src)\n",
    "        \n",
    "        # Pass through each Encoder Layer\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        if self.norm_first:\n",
    "            src = self.layernorm(src)\n",
    "        \n",
    "        return src\n",
    "\n",
    "sample = torch.randint(0, src_vocab_size, (batch_size, max_len))\n",
    "encoder = Encoder(src_vocab_size, d_model, n_heads, d_pff, max_len, n_layers, dropout, norm_first=True)\n",
    "\n",
    "enc_out = encoder(sample)\n",
    "enc_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100, 512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_pff, dropout, norm_first=True):\n",
    "        super().__init__()\n",
    "        self.norm_first = norm_first\n",
    "        \n",
    "        # Masked Multi-Head Attention layer\n",
    "        self.masked_mha = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        \n",
    "        # Encoder-Decoder Multi-Head Attention layer\n",
    "        self.enc_dec_mha = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        \n",
    "        # Pointwise Feed-Forward layer\n",
    "        self.pff = PointwiseFeedForward(d_model, d_pff, dropout)\n",
    "        \n",
    "        # Layer Normalization layers\n",
    "        self.layernorm_1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm_2 = nn.LayerNorm(d_model)\n",
    "        self.layernorm_3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, tgt, enc_out, src_mask=None, tgt_mask=None):\n",
    "        if self.norm_first:\n",
    "            norm_tgt = self.layernorm_1(tgt)\n",
    "            masked_attn_out = tgt + self.masked_mha(norm_tgt, norm_tgt, norm_tgt, tgt_mask)\n",
    "            \n",
    "            norm_masked_attn_out = self.layernorm_2(masked_attn_out)\n",
    "            enc_dec_attn_out = masked_attn_out + self.enc_dec_mha(norm_masked_attn_out, enc_out, enc_out, src_mask)\n",
    "            \n",
    "            norm_enc_dec_attn_out = self.layernorm_3(enc_dec_attn_out)\n",
    "            pff_out = enc_dec_attn_out + self.pff(norm_enc_dec_attn_out)\n",
    "        else:\n",
    "            masked_attn_out = self.masked_mha(tgt, tgt, tgt, tgt_mask)\n",
    "            masked_attn_out = self.layernorm_1(tgt + masked_attn_out)\n",
    "            \n",
    "            enc_dec_attn_out = self.enc_dec_mha(masked_attn_out, enc_out, enc_out, src_mask)\n",
    "            enc_dec_attn_out = self.layernorm_2(masked_attn_out + enc_dec_attn_out)\n",
    "            \n",
    "            pff_out = self.pff(enc_dec_attn_out)\n",
    "            pff_out = self.layernorm_3(enc_dec_attn_out + pff_out)\n",
    "        \n",
    "        return pff_out\n",
    "\n",
    "\n",
    "dec_layer = DecoderLayer(d_model, n_heads, d_pff, dropout)\n",
    "dec_layer(output, enc_layer_out).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, d_model, n_heads, d_pff, dropout, n_layers, norm_first=True):\n",
    "        super().__init__()\n",
    "        self.norm_first = norm_first\n",
    "        if norm_first:\n",
    "            self.layernorm = nn.LayerNorm(d_model)\n",
    "            \n",
    "        self.tgt_embedding = TokenEmbedding(tgt_vocab_size, d_model, max_len, dropout)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_heads, d_pff, dropout, norm_first) for _ in range(n_layers)\n",
    "        ])\n",
    "            \n",
    "    def forward(self, tgt, enc_out, src_mask=None, tgt_mask=None):\n",
    "        tgt = self.tgt_embedding(tgt)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, enc_out, src_mask, tgt_mask)\n",
    "        if self.norm_first:\n",
    "            tgt = self.layernorm(tgt)\n",
    "            \n",
    "        return tgt\n",
    "    \n",
    "tgt_sample = torch.randint(0, tgt_vocab_size, (batch_size, max_len))\n",
    "print(enc_out.shape)\n",
    "decoder = Decoder(tgt_vocab_size, d_model, n_heads, d_pff, dropout, n_layers, norm_first=True)\n",
    "dec_out = decoder(tgt_sample, enc_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, tgt_vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100, 1500])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_pff,\n",
    "                 max_len,\n",
    "                 n_layers,\n",
    "                 dropout,\n",
    "                 norm_first=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(src_vocab_size, d_model, n_heads, d_pff, max_len, n_layers, dropout, norm_first)\n",
    "        self.decoder = Decoder(tgt_vocab_size, d_model, n_heads, d_pff, dropout, n_layers, norm_first)\n",
    "        self.generator = Generator(d_model, tgt_vocab_size)\n",
    "        \n",
    "    def create_masks(self, src, tgt):\n",
    "        # Source Mask\n",
    "        # Shape: (batch_size, 1, 1, max_len)\n",
    "        src_mask = (src == 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Target Mask\n",
    "        # Shape: (batch_size, 1, max_len, max_len)\n",
    "        tgt_mask = torch.triu(torch.ones((tgt.size(1), tgt.size(1))), diagonal=1).bool()\n",
    "        tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(1)\n",
    "        tgt_padding_mask = (tgt == 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = tgt_mask & tgt_padding_mask\n",
    "\n",
    "        return src_mask, tgt_mask\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.create_masks(src, tgt)\n",
    "        \n",
    "        enc_out = self.encoder(src, src_mask)\n",
    "        dec_out = self.decoder(tgt, enc_out, src_mask, tgt_mask)\n",
    "        \n",
    "        output = self.generator(dec_out)\n",
    "        \n",
    "        return output\n",
    "\n",
    "model = Transformer(src_vocab_size, 1500, d_model, n_heads, d_pff, max_len, n_layers, dropout, norm_first=True)\n",
    "src_sample = torch.randint(0, src_vocab_size, (batch_size, max_len))\n",
    "tgt_sample = torch.randint(0, 1500, (batch_size, max_len))\n",
    "model(src_sample, tgt_sample).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
